# Дипломная работа по профессии Инженер данных

### 1. Исследование предоставленного [датасета](https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales?select=supermarket_sales+-+Sheet1.csv)

[Первичный анализ датасета](/dataset_research/research_notebook.ipynb) показал что:

 - атрибуты **date** и **time** являются строками, хотя по факту это велечины времени

- **date** и **time** из строк можно привести в формат одного поля типа создать *datetime*

- каждый филиал находится только в одном городе

- поля **Customer type** и **Gender** зависят только от заказа

- отсутствует сущность **Продукт**, но может появиться в будущем

- **gross income** совпадает с **Tax 5%**

- **gross margin percentage** - константа, одинаковая для всех записей
- данный датасет можно скачать через url из [репозитеория](https://raw.githubusercontent.com/sushantag9/Supermarket-Sales-Data-Analysis/master/supermarket_sales%20-%20Sheet1.csv) на github.

### 2. Построение моделей данных для базы данных и хранилища данных

*рис. 1 Схема нормализованного хранилища данных*

![](/project_design/nds.jpg)

Так как в предложенном датасете все значения атомарные, то датасет уже находится в первой нормальной форме.

Далее, я выделил следующие сущности:

Invoices, Customer_types, Product_lines, Branches, Cities, Payment_types. 

Исходя из датасета, я установил зависимость между **Branches** и **Cities** как одна к одному, однако в будущем это может поменяться - один или несколько филиалов могут добавить города. 

Остальные сущности зависят исключительно от заказов, но могут быть изменения, особенно это касается сущностей **Customer_types** и **Product_lines** - высока вероятность появления появления сущностей **Customer** и **Product**. 

*рис. 2 Схема хранилища данных типа "звезда"*

![](/project_design/dds.jpg)

Выше приведена схема предполагаемого аналитического хранилища данных. Однако, на мой взгляд, такая реализация нецелесообразна ввиду малого количества атрибутов. 

Наиболее предпочтительный способ реализации проекта, на мой взгляд - это создание витрины в нормализованном хранилище, откуда данные будут забираться в аналитическое хранилище и хранится там в плоской таблице. В качестве такого хранилища я выбрал **Clickhouse**. 


### 3. ETL проекта

#### Предположения:

- дан внешний источник данных, откуда можно забирать обновлённые данные в **N** период времени

- необходимо сделать прототип ETL процесса

- безопасность и хранение секретных данных не являются частью проекта и базах данных могут использоваться пароли и пользователи по умолчанию

*рис. 3 Схема ETL проекта*

![](/project_design/etl.png)

**ETL** проекта будет выполнен следующим образом:

- данные собираются из стороннего источника, обрабатываются и загружаются в нормализованное хранилище на базе **Postgres**, где создаётся витрина для аналитического потребителя. 

- далее, данные из витрины в нормализованном хранилище забираются и загружаются в аналитическое хранилище.

Оркестрация процессов выполняется в **Apache Airflow**, инфраструктура прототипа проекта развёрнута в **Docker**.

Все файлы для данного этапа работы находятся в папке [**ETL**](/ETL/)

Ифраструктура может быть развёрнута командой :

```docker-compose up --build -d```

Скрипты развёртывания баз данных [нормализованное хранилище](/ETL/init_scripts/init_nds.sql) и [аналитическое хранилище](/ETL/init_scripts/init_dwh.sql) находятся в директории **init_scripts**.

Всего в airflow три дага. 
- Даг [to_nds_dag.py](/ETL/dags/to_nds_dag.py) отвечает за забор данных из внешнего источника и запись в нормализованное хранилище. Ввиду того, что хорошей практикой считается не использовать **PythonOperator** для трансформации и работы с данными, так как он будет использовать ресурсы airflow, я использовал **BashOpertator** и скрипт [from_url_to_nds.py](/ETL/dags/etl_scripts/from_url_to_nds.py) для получения данных из источника и их записи в нормализованное хранилище. Данный скрипт использует библиотеку **pandas** для трансформации данных.

- Даг [to_dwh_dag.py](/ETL/dags/to_nds_dag.py) отвечает за забор данных из нормализованного хранилища и запись в аналитическое хранилище. По причинам, описанным выше, данный даг запускает скрипт [from_nds_to_dwh.py](/ETL/dags/etl_scripts/from_nds_to_dwh.py)

- Даг [dq_check_dag.py](/ETL/dags/dq_check_dag.py) отвечает за базовую проверку качества данных. Реализована проверка уникальности поля Invoice ID и проверка на отсутвие нулевые и отрицательных значений для количественных переменных. 

**P.S.** в качестве хранения секретов для проекта использовался файл secrets.json - это не production решение.

### 4. Визуализация данных

Визуализацию данных в **Tableau** можно найти по [ссылке](https://public.tableau.com/app/profile/anatolii1241/viz/netology_de_diploma/Dashboard1?publish=yes)

*рис. 4 Визуализация данных*
![](/dataset_research/dataviz.png)

Как можно увидеть из дашборда:

- женщины тратят больше на еду и на питки, чем мужчины

- мужчины заметно больше тратят на красоту и здоровье

- женщины больше тратили в магазинах филиала C, а мужчины в магазинах филиалов A и B

- еда и напитки принесли больше всего выручки в филиале С, в филиале A - категория дом и образ жизни, а в филиале B спорт и путешествия и красота и здоровье

- в феврале было заметное снижение выручки по категориям спорт и путешествия, и дом и образ жизни
